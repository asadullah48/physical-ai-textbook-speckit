"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[87],{7785(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-1-intro/sensor-systems","title":"Sensor Systems","description":"Understanding sensors in robotics - cameras, LiDAR, and IMUs","source":"@site/docs/module-1-intro/02-sensor-systems.mdx","sourceDirName":"module-1-intro","slug":"/module-1-intro/sensor-systems","permalink":"/physical-ai-textbook-speckit/docs/module-1-intro/sensor-systems","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Sensor Systems","description":"Understanding sensors in robotics - cameras, LiDAR, and IMUs","keywords":["sensors","lidar","camera","imu","perception"],"week":1,"estimated_time":50,"difficulty":"beginner","learning_objectives":["Explain the role of different sensors in robotic perception","Compare and contrast camera, LiDAR, and IMU technologies","Understand sensor fusion concepts","Choose appropriate sensors for different applications"],"prerequisites":["module-1-intro/01-what-is-physical-ai"]},"sidebar":"tutorialSidebar","previous":{"title":"What is Physical AI?","permalink":"/physical-ai-textbook-speckit/docs/module-1-intro/what-is-physical-ai"},"next":{"title":"Embodied Intelligence","permalink":"/physical-ai-textbook-speckit/docs/module-1-intro/embodied-intelligence"}}');var t=s(4848),r=s(8453);const a={sidebar_position:2,title:"Sensor Systems",description:"Understanding sensors in robotics - cameras, LiDAR, and IMUs",keywords:["sensors","lidar","camera","imu","perception"],week:1,estimated_time:50,difficulty:"beginner",learning_objectives:["Explain the role of different sensors in robotic perception","Compare and contrast camera, LiDAR, and IMU technologies","Understand sensor fusion concepts","Choose appropriate sensors for different applications"],prerequisites:["module-1-intro/01-what-is-physical-ai"]},o="Sensor Systems for Physical AI",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Camera Systems",id:"camera-systems",level:2},{value:"RGB Cameras",id:"rgb-cameras",level:3},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"LiDAR Sensors",id:"lidar-sensors",level:2},{value:"LiDAR vs Camera Comparison",id:"lidar-vs-camera-comparison",level:3},{value:"Inertial Measurement Units (IMU)",id:"inertial-measurement-units-imu",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Easy",id:"easy",level:3},{value:"Medium",id:"medium",level:3},{value:"Challenging",id:"challenging",level:3}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"sensor-systems-for-physical-ai",children:"Sensor Systems for Physical AI"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Explain the role of different sensors in robotic perception"}),"\n",(0,t.jsx)(n.li,{children:"Compare and contrast camera, LiDAR, and IMU technologies"}),"\n",(0,t.jsx)(n.li,{children:"Understand sensor fusion concepts"}),"\n",(0,t.jsx)(n.li,{children:"Choose appropriate sensors for different applications"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:'Sensors are the "eyes and ears" of a Physical AI system. They convert physical phenomena into electrical signals that can be processed by computers. The quality and reliability of sensor data fundamentally limits what a robot can perceive and accomplish.'}),"\n",(0,t.jsx)(n.h2,{id:"camera-systems",children:"Camera Systems"}),"\n",(0,t.jsx)(n.h3,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,t.jsx)(n.p,{children:"Standard RGB cameras capture color images similar to human vision:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Tuple\n\n@dataclass\nclass CameraIntrinsics:\n    """Camera intrinsic parameters."""\n    fx: float  # Focal length x (pixels)\n    fy: float  # Focal length y (pixels)\n    cx: float  # Principal point x (pixels)\n    cy: float  # Principal point y (pixels)\n\n    def project_point(self, point_3d: np.ndarray) -> Tuple[float, float]:\n        """Project a 3D point to 2D image coordinates."""\n        x, y, z = point_3d\n        u = (self.fx * x / z) + self.cx\n        v = (self.fy * y / z) + self.cy\n        return u, v\n\n# Example: Intel RealSense D435 camera intrinsics\nrealsense_intrinsics = CameraIntrinsics(\n    fx=615.0,\n    fy=615.0,\n    cx=320.0,\n    cy=240.0\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,t.jsx)(n.p,{children:"Depth cameras provide per-pixel distance measurements:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def process_depth_image(depth_frame: np.ndarray,\n                        intrinsics: CameraIntrinsics,\n                        max_depth: float = 10.0) -> np.ndarray:\n    """Convert depth image to 3D point cloud."""\n    height, width = depth_frame.shape\n\n    # Create pixel coordinate grids\n    u = np.arange(width)\n    v = np.arange(height)\n    u, v = np.meshgrid(u, v)\n\n    # Calculate 3D coordinates\n    z = depth_frame.astype(np.float32) / 1000.0  # mm to meters\n    x = (u - intrinsics.cx) * z / intrinsics.fx\n    y = (v - intrinsics.cy) * z / intrinsics.fy\n\n    # Stack into point cloud\n    points = np.stack([x, y, z], axis=-1)\n\n    # Filter invalid depths\n    valid_mask = (z > 0) & (z < max_depth)\n    return points[valid_mask]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"lidar-sensors",children:"LiDAR Sensors"}),"\n",(0,t.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) uses laser pulses to measure precise distances:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass\nfrom typing import List\nimport numpy as np\n\n@dataclass\nclass LiDARScan:\n    """Represents a LiDAR scan."""\n    points: np.ndarray      # Shape: (N, 3) for x, y, z\n    intensities: np.ndarray # Shape: (N,) reflectivity values\n    timestamp: float\n\n    def filter_by_range(self, min_range: float, max_range: float) -> \'LiDARScan\':\n        """Filter points by distance from sensor."""\n        distances = np.linalg.norm(self.points, axis=1)\n        mask = (distances >= min_range) & (distances <= max_range)\n        return LiDARScan(\n            points=self.points[mask],\n            intensities=self.intensities[mask],\n            timestamp=self.timestamp\n        )\n\n    def remove_ground_plane(self, height_threshold: float = 0.2) -> \'LiDARScan\':\n        """Simple ground plane removal based on height."""\n        mask = self.points[:, 2] > height_threshold\n        return LiDARScan(\n            points=self.points[mask],\n            intensities=self.intensities[mask],\n            timestamp=self.timestamp\n        )\n'})}),"\n",(0,t.jsx)(n.h3,{id:"lidar-vs-camera-comparison",children:"LiDAR vs Camera Comparison"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Feature"}),(0,t.jsx)(n.th,{children:"Camera"}),(0,t.jsx)(n.th,{children:"LiDAR"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Range"})}),(0,t.jsx)(n.td,{children:"Limited (10-50m)"}),(0,t.jsx)(n.td,{children:"Excellent (100m+)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Accuracy"})}),(0,t.jsx)(n.td,{children:"Low depth accuracy"}),(0,t.jsx)(n.td,{children:"Centimeter precision"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Lighting"})}),(0,t.jsx)(n.td,{children:"Affected by lighting"}),(0,t.jsx)(n.td,{children:"Works in darkness"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Cost"})}),(0,t.jsx)(n.td,{children:"Low ($50-500)"}),(0,t.jsx)(n.td,{children:"High ($1k-100k)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Data rate"})}),(0,t.jsx)(n.td,{children:"30-60 Hz"}),(0,t.jsx)(n.td,{children:"10-20 Hz"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Density"})}),(0,t.jsx)(n.td,{children:"Very high (megapixels)"}),(0,t.jsx)(n.td,{children:"Sparse (thousands of points)"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"inertial-measurement-units-imu",children:"Inertial Measurement Units (IMU)"}),"\n",(0,t.jsx)(n.p,{children:"IMUs measure linear acceleration and angular velocity:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\n@dataclass\nclass IMUData:\n    """IMU measurement data."""\n    accel: np.ndarray  # Linear acceleration (m/s\xb2)\n    gyro: np.ndarray   # Angular velocity (rad/s)\n    timestamp: float\n\nclass IMUIntegrator:\n    """Integrates IMU data for pose estimation."""\n\n    def __init__(self):\n        self.orientation = Rotation.identity()\n        self.velocity = np.zeros(3)\n        self.position = np.zeros(3)\n        self.last_timestamp = None\n\n    def update(self, imu: IMUData) -> None:\n        """Update pose estimate with new IMU reading."""\n        if self.last_timestamp is None:\n            self.last_timestamp = imu.timestamp\n            return\n\n        dt = imu.timestamp - self.last_timestamp\n\n        # Update orientation (integrate gyroscope)\n        delta_rotation = Rotation.from_rotvec(imu.gyro * dt)\n        self.orientation = self.orientation * delta_rotation\n\n        # Remove gravity from accelerometer\n        gravity = np.array([0, 0, 9.81])\n        world_accel = self.orientation.apply(imu.accel) - gravity\n\n        # Update velocity and position\n        self.velocity += world_accel * dt\n        self.position += self.velocity * dt\n\n        self.last_timestamp = imu.timestamp\n\n    def get_pose(self) -> dict:\n        """Get current pose estimate."""\n        return {\n            "position": self.position.tolist(),\n            "orientation": self.orientation.as_quat().tolist(),\n            "velocity": self.velocity.tolist()\n        }\n'})}),"\n",(0,t.jsx)(n.admonition,{title:"IMU Drift",type:"warning",children:(0,t.jsx)(n.p,{children:"Raw IMU integration suffers from drift over time due to sensor noise and bias. In practice, IMU data must be fused with other sensors (camera, LiDAR, GPS) for accurate long-term pose estimation."})}),"\n",(0,t.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,t.jsx)(n.p,{children:"Combining multiple sensors improves reliability and accuracy:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from typing import Optional\nimport numpy as np\n\nclass SensorFusion:\n    """Simple complementary filter for sensor fusion."""\n\n    def __init__(self, alpha: float = 0.98):\n        """\n        Args:\n            alpha: Weight for high-frequency sensor (gyro).\n                   1-alpha is weight for low-frequency sensor (accel).\n        """\n        self.alpha = alpha\n        self.roll = 0.0\n        self.pitch = 0.0\n\n    def update(self, accel: np.ndarray, gyro: np.ndarray, dt: float) -> tuple:\n        """\n        Fuse accelerometer and gyroscope data.\n\n        Args:\n            accel: Accelerometer reading (x, y, z) in m/s\xb2\n            gyro: Gyroscope reading (x, y, z) in rad/s\n            dt: Time delta in seconds\n\n        Returns:\n            Tuple of (roll, pitch) in radians\n        """\n        # Roll and pitch from accelerometer\n        accel_roll = np.arctan2(accel[1], accel[2])\n        accel_pitch = np.arctan2(-accel[0],\n                                  np.sqrt(accel[1]**2 + accel[2]**2))\n\n        # Integrate gyroscope\n        self.roll += gyro[0] * dt\n        self.pitch += gyro[1] * dt\n\n        # Complementary filter\n        self.roll = self.alpha * self.roll + (1 - self.alpha) * accel_roll\n        self.pitch = self.alpha * self.pitch + (1 - self.alpha) * accel_pitch\n\n        return self.roll, self.pitch\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cameras"})," provide rich visual information but struggle with depth and lighting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LiDAR"})," offers precise 3D measurements but is expensive and sparse"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"IMUs"})," track motion at high frequency but suffer from drift"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor fusion"})," combines strengths of multiple sensors"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"easy",children:"Easy"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"What are the main components of an IMU?"}),"\n",(0,t.jsx)(n.li,{children:"List three advantages of LiDAR over cameras."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"medium",children:"Medium"}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Implement a function that converts a depth image to a colorized visualization."}),"\n",(0,t.jsxs)(n.li,{children:["Modify the ",(0,t.jsx)(n.code,{children:"LiDARScan"})," class to include a method for detecting clusters of points."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"challenging",children:"Challenging"}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsx)(n.li,{children:"Design a sensor suite for a delivery robot operating outdoors. Justify your choices."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>a,x:()=>o});var i=s(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);